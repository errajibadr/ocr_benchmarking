#!/usr/bin/env python3
"""
OCR Benchmarking Comparison - Script for Google Colab
Copy and paste sections of this code into a Google Colab notebook
"""

# %% [markdown]
# # OCR Benchmarking Comparison
# 
# This notebook compares different OCR methods against ground truth generated by Gemini 2.5 Flash.
# 
# ## Setup

# %%
# Install dependencies
!pip install pytesseract opencv-python easyocr paddlepaddle paddleocr keras-ocr matplotlib seaborn tqdm
!apt-get install tesseract-ocr -y

# %% [markdown]
# ## Upload Images and Ground Truth
# Upload the sample dataset and ground truth JSON file:

# %%
import os

# Create directories
!mkdir -p dataset/sample/images

# Option 1: Upload images using Colab file uploader
from google.colab import files
import shutil

print("Please upload your ground_truth.json file...")
uploaded = files.upload()
for filename in uploaded.keys():
    shutil.move(filename, f"dataset/sample/{filename}")
    
print("Please upload your image files...")
uploaded = files.upload()
for filename in uploaded.keys():
    shutil.move(filename, f"dataset/sample/images/{filename}")

# %% [markdown]
# ## Define OCR Methods
# Let's create the file containing our OCR methods:

# %%
# Define OCR methods

# 1. Tesseract OCR
def ocr_tesseract(image_path: str) -> str:
    """Extract text from image using Tesseract OCR

    Installation: !pip install pytesseract opencv-python
    Note: Also requires tesseract to be installed on the system.
          In Colab: !apt-get install tesseract-ocr
    """
    import cv2
    import pytesseract

    # Read image using OpenCV
    img = cv2.imread(image_path)

    # Convert to grayscale
    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)

    # Apply threshold to get image with only black and white
    _, binary = cv2.threshold(gray, 150, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)

    # Recognize text with Tesseract
    text = pytesseract.image_to_string(binary)

    return text


# 2. EasyOCR
def ocr_easyocr(image_path: str) -> str:
    """Extract text from image using EasyOCR

    Installation: !pip install easyocr
    """
    import easyocr

    # Initialize reader for English
    reader = easyocr.Reader(["en"])

    # Read text
    result = reader.readtext(image_path)

    # Combine text from all detected regions
    text = "\n".join([item[1] for item in result])

    return text


# 3. PaddleOCR
def ocr_paddleocr(image_path: str) -> str:
    """Extract text from image using PaddleOCR

    Installation: !pip install paddlepaddle paddleocr
    """
    from paddleocr import PaddleOCR

    # Initialize PaddleOCR
    ocr = PaddleOCR(use_angle_cls=True, lang="en")

    # Process image
    result = ocr.ocr(image_path, cls=True)

    # Extract text
    text_lines = []
    if result[0] is not None:  # Check if any results were found
        for line in result[0]:
            if len(line) >= 2:  # Ensure we have the text part
                text_lines.append(line[1][0])  # Get text content

    # Join all lines
    text = "\n".join(text_lines)

    return text


# 4. Keras OCR (for simple text detection)
def ocr_kerasocr(image_path: str) -> str:
    """Extract text from image using Keras OCR

    Installation: !pip install keras-ocr
    """
    import keras_ocr

    # Initialize detector and recognizer
    pipeline = keras_ocr.pipeline.Pipeline()

    # Read image
    images = [keras_ocr.tools.read(image_path)]

    # Make prediction
    predictions = pipeline.recognize(images)

    # Extract text with rough position information
    text_with_positions = []
    for prediction in predictions[0]:
        word, box = prediction
        # Calculate rough position (top-left of box)
        x, y = box[0][0], box[0][1]
        text_with_positions.append((y, x, word))

    # Sort by vertical position first (top to bottom)
    text_with_positions.sort()

    # Group words that are roughly on the same line
    line_height = 20  # Adjust based on image resolution
    lines = []
    current_line = []
    current_y = None

    for y, x, word in text_with_positions:
        if current_y is None or abs(y - current_y) < line_height:
            current_line.append((x, word))
            current_y = y
        else:
            # Sort words in the current line by horizontal position (left to right)
            current_line.sort()
            lines.append(" ".join(word for _, word in current_line))
            current_line = [(x, word)]
            current_y = y

    # Add the last line
    if current_line:
        current_line.sort()
        lines.append(" ".join(word for _, word in current_line))

    return "\n".join(lines)


# Dictionary mapping method names to functions
OCR_METHODS = {
    "tesseract": ocr_tesseract,
    "easyocr": ocr_easyocr,
    "paddleocr": ocr_paddleocr,
    "kerasocr": ocr_kerasocr,
}

# %% [markdown]
# ## Define Comparison Framework

# %%
# Define comparison framework
import json
import os
import pathlib
import time
from dataclasses import dataclass
from typing import Dict, List, Optional
import seaborn as sns
import matplotlib.pyplot as plt
from tqdm import tqdm


@dataclass
class OCRResult:
    """Stores the result of an OCR method along with performance metrics"""

    method_name: str
    extracted_text: Dict[str, str]
    processing_time: Dict[str, float]


def load_ground_truth(path: str = "dataset/sample/ground_truth.json") -> Dict[str, str]:
    """Load the ground truth data from Gemini API"""
    with open(path, "r", encoding="utf-8") as f:
        return json.load(f)


def process_dataset(
    image_dir: str, ocr_methods: List, sample_limit: Optional[int] = None
) -> List[OCRResult]:
    """Process all images in the dataset with each OCR method

    Args:
        image_dir: Directory containing images to process
        ocr_methods: List of OCR method functions to apply
        sample_limit: Optional limit on number of images to process

    Returns:
        List of OCRResult for each method
    """
    results = []
    image_paths = list(pathlib.Path(image_dir).glob("*.png"))

    if sample_limit and sample_limit < len(image_paths):
        image_paths = image_paths[:sample_limit]

    for method in ocr_methods:
        extracted_text = {}
        processing_time = {}

        for img_path in tqdm(image_paths, desc=f"Processing with {method.__name__}"):
            start_time = time.time()
            try:
                text = method(str(img_path))
                elapsed = time.time() - start_time

                extracted_text[img_path.name] = text
                processing_time[img_path.name] = elapsed
            except Exception as e:
                print(f"Error processing {img_path.name} with {method.__name__}: {str(e)}")
                extracted_text[img_path.name] = f"ERROR: {str(e)}"
                processing_time[img_path.name] = -1

        results.append(
            OCRResult(
                method_name=method.__name__,
                extracted_text=extracted_text,
                processing_time=processing_time,
            )
        )

    return results


def evaluate_results(results: List[OCRResult], ground_truth: Dict[str, str]) -> Dict:
    """Evaluates OCR results against ground truth

    For demonstration purposes, this just compares text length and processing time
    """
    evaluation = {}

    for result in results:
        method_stats = {
            "avg_processing_time": 0,
            "text_length_ratio": {},
            "file_counts": {"success": 0, "error": 0},
        }

        # Calculate metrics
        valid_times = [t for t in result.processing_time.values() if t > 0]
        if valid_times:
            method_stats["avg_processing_time"] = sum(valid_times) / len(valid_times)

        for filename, text in result.extracted_text.items():
            if filename in ground_truth and not text.startswith("ERROR:"):
                if ground_truth[filename]:
                    # Simple ratio of extracted text length to ground truth length
                    ratio = len(text) / len(ground_truth[filename])
                    method_stats["text_length_ratio"][filename] = ratio
                    method_stats["file_counts"]["success"] += 1
                else:
                    method_stats["text_length_ratio"][filename] = 0
                    method_stats["file_counts"]["error"] += 1
            else:
                method_stats["file_counts"]["error"] += 1

        evaluation[result.method_name] = method_stats

    return evaluation


def save_results(results: List[OCRResult], output_dir: str = "results"):
    """Save OCR results to json files"""
    os.makedirs(output_dir, exist_ok=True)

    for result in results:
        output_file = os.path.join(output_dir, f"{result.method_name}.json")

        with open(output_file, "w", encoding="utf-8") as f:
            json.dump(result.extracted_text, f, indent=2, ensure_ascii=False)

        print(f"Saved results for {result.method_name} to {output_file}")


def visualize_results(evaluation: Dict, output_dir: str = "results"):
    """Create visualizations for OCR comparison"""
    os.makedirs(output_dir, exist_ok=True)

    # Processing time comparison
    methods = list(evaluation.keys())
    proc_times = [evaluation[m]["avg_processing_time"] for m in methods]

    plt.figure(figsize=(10, 6))
    sns.barplot(x=methods, y=proc_times)
    plt.title("Average Processing Time by OCR Method")
    plt.ylabel("Time (seconds)")
    plt.xticks(rotation=45)
    plt.tight_layout()
    plt.savefig(os.path.join(output_dir, "processing_time_comparison.png"))
    plt.show()

    # Text length ratio comparison (boxplot)
    plt.figure(figsize=(10, 6))
    data = []
    labels = []

    for method in methods:
        ratios = list(evaluation[method]["text_length_ratio"].values())
        if ratios:  # Only add if we have data
            data.append(ratios)
            labels.append(method)

    if data:
        plt.boxplot(data)
        plt.xticks(range(1, len(labels) + 1), labels)
        plt.title("Text Length Ratio (Extracted / Ground Truth)")
        plt.ylabel("Ratio")
        plt.axhline(y=1.0, color="r", linestyle="--")
        plt.xticks(rotation=45)
        plt.tight_layout()
        plt.savefig(os.path.join(output_dir, "text_length_comparison.png"))
        plt.show()

    # Success rate
    success_rates = []
    for m in methods:
        success = evaluation[m]["file_counts"]["success"]
        error = evaluation[m]["file_counts"]["error"]
        total = success + error
        if total > 0:
            success_rates.append(success / total)
        else:
            success_rates.append(0)

    plt.figure(figsize=(10, 6))
    sns.barplot(x=methods, y=success_rates)
    plt.title("Success Rate by OCR Method")
    plt.ylabel("Success Rate")
    plt.xticks(rotation=45)
    plt.tight_layout()
    plt.savefig(os.path.join(output_dir, "success_rate_comparison.png"))
    plt.show()

# %% [markdown]
# ## Run the Comparison

# %%
# Run the comparison
# Configure parameters
IMAGE_DIR = "dataset/sample/images"
GROUND_TRUTH_PATH = "dataset/sample/ground_truth.json"

# Load ground truth data
print("Loading ground truth data...")
ground_truth = load_ground_truth(GROUND_TRUTH_PATH)
print(f"Loaded ground truth for {len(ground_truth)} images")

# Process images with all OCR methods
print("\nProcessing images with OCR methods...")
methods_to_use = [
    OCR_METHODS["tesseract"],
    OCR_METHODS["easyocr"],
    OCR_METHODS["paddleocr"],
    OCR_METHODS["kerasocr"],
]

results = process_dataset(IMAGE_DIR, methods_to_use)

# Save results
print("\nSaving results...")
save_results(results)

# Evaluate results
print("\nEvaluating results...")
evaluation = evaluate_results(results, ground_truth)

# Visualize comparison
print("\nCreating visualizations...")
visualize_results(evaluation)

# %% [markdown]
# ## Text Quality Comparison
# 
# Let's compare the actual extracted text for a single image:

# %%
# Text quality comparison
import random
import os

# Get list of image files
image_files = os.listdir("dataset/sample/images")

# Select a random image to compare
random_image = random.choice(image_files)
print(f"Selected image for comparison: {random_image}\n")

# Display the image
from IPython.display import Image
display(Image(filename=f"dataset/sample/images/{random_image}"))

print("\n" + "-"*80)
print("GROUND TRUTH:")
print("-"*80)
print(ground_truth[random_image])

# Print extracted text from each method
for result in results:
    print("\n" + "-"*80)
    print(f"{result.method_name.upper()}:")
    print("-"*80)
    print(result.extracted_text[random_image])
    print(f"Processing time: {result.processing_time[random_image]:.2f} seconds")

# %% [markdown]
# ## Text Similarity Metrics
# 
# Let's compute some similarity metrics to compare the text quality:

# %%
# Text similarity metrics
from difflib import SequenceMatcher

def text_similarity(a, b):
    """Compute similarity ratio between two strings using SequenceMatcher"""
    return SequenceMatcher(None, a, b).ratio()

# Calculate similarity for each method and image
similarity_scores = {}

for result in results:
    method_scores = {}
    for filename, extracted_text in result.extracted_text.items():
        if filename in ground_truth and not extracted_text.startswith("ERROR"):
            # Calculate similarity score
            truth = ground_truth[filename]
            score = text_similarity(extracted_text, truth)
            method_scores[filename] = score
    
    similarity_scores[result.method_name] = method_scores

# Create a plot of average similarity scores
import numpy as np

method_names = list(similarity_scores.keys())
avg_scores = [np.mean(list(scores.values())) for method_name, scores in similarity_scores.items()]

plt.figure(figsize=(10, 6))
sns.barplot(x=method_names, y=avg_scores)
plt.title("Average Text Similarity Score (compared to Gemini ground truth)")
plt.ylabel("Similarity Score (0-1)")
plt.xticks(rotation=45)
plt.tight_layout()
plt.savefig("results/similarity_comparison.png")
plt.show()

# Print average similarity scores
print("Average similarity scores (higher is better):")
for method, score in zip(method_names, avg_scores):
    print(f"{method}: {score:.4f}")

# %% [markdown]
# ## Conclusion
# 
# This notebook compared several OCR methods against the ground truth generated by Gemini 2.5 Flash. Based on the results, we can see which methods perform best on this particular dataset.
# 
# Summary of findings:
# 1. **Text quality**: Similarity scores show how close each method comes to the Gemini ground truth
# 2. **Processing time**: Different methods have different speed tradeoffs
# 3. **Success rate**: Some methods may fail on certain images 